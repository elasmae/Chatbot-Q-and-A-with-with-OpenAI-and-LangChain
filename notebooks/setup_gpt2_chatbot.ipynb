{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a722ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EL mahjoubi\\Desktop\\Chatbot Q&A\\chatenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Introduction_to_Tableau.pdf\")\n",
    "docs = loader.load()\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs_split = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(text, convert_to_tensor=False).tolist()\n",
    "\n",
    "embedding = SentenceTransformerEmbeddings(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs_split,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"lambda\": 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d59cf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\EL mahjoubi\\AppData\\Local\\Temp\\ipykernel_11084\\537155765.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  chat = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    truncation=True,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "chat = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(prompt: str, max_tokens: int = 900):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CREATING_QUESTION = '''Lecture: {question_lecture}\n",
    "Title: {question_title}\n",
    "Body: {question_body}'''\n",
    "\n",
    "PROMPT_RETRIEVING_S = '''You will receive a question from a student taking a Tableau course, which includes a title and a body. \n",
    "The corresponding lecture will also be provided.\n",
    "\n",
    "Answer the question using only the provided context.\n",
    "\n",
    "At the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \n",
    "Resources: \n",
    "Section: *Section Title*, Lecture: *Lecture Title*'''\n",
    "\n",
    "PROMPT_TEMPLATE_RETRIEVING_H = '''This is the question:\n",
    "{question}\n",
    "\n",
    "This is the context:\n",
    "{context}'''\n",
    "\n",
    "prompt_creating_question = PromptTemplate.from_template(PROMPT_CREATING_QUESTION)\n",
    "prompt_retrieving_s = SystemMessage(content=PROMPT_RETRIEVING_S)\n",
    "prompt_template_retrieving_h = HumanMessagePromptTemplate.from_template(PROMPT_TEMPLATE_RETRIEVING_H)\n",
    "chat_prompt_template_retrieving = ChatPromptTemplate(messages=[prompt_retrieving_s, prompt_template_retrieving_h])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(dictionary):\n",
    "    formatted_string = \"\"\n",
    "    retrieved_list = dictionary['context']\n",
    "\n",
    "    for i, doc in enumerate(retrieved_list):\n",
    "        section = doc.metadata.get(\"section_title\", \"Unknown Section\")\n",
    "        lecture = doc.metadata.get(\"lecture_title\", \"Unknown Lecture\")\n",
    "        formatted_string += f'''\n",
    "Document {i + 1}\n",
    "Section Title: {section}\n",
    "Lecture Title: {lecture}\n",
    "Content: {doc.page_content.strip()}\n",
    "\n",
    "-------------------\n",
    "'''\n",
    "    new_dictionary = dictionary.copy()\n",
    "    new_dictionary['context'] = formatted_string\n",
    "    return new_dictionary\n",
    "\n",
    "format_context_runnable = RunnableLambda(format_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text = RunnableLambda(lambda x: x.to_string())\n",
    "combine_with_context = RunnableLambda(lambda q: {\n",
    "    \"context\": retriever.invoke(q),\n",
    "    \"question\": q\n",
    "})\n",
    "apply_truncation = RunnableLambda(lambda d: {\n",
    "    \"question\": d[\"question\"],\n",
    "    \"context\": truncate_prompt(d[\"context\"])\n",
    "})\n",
    "final_prompt_truncation = RunnableLambda(lambda messages: truncate_prompt(str(messages)))\n",
    "\n",
    "chain_retrieving_improved = (\n",
    "    prompt_creating_question\n",
    "    | get_text\n",
    "    | combine_with_context\n",
    "    | format_context_runnable\n",
    "    | apply_truncation\n",
    "    | chat_prompt_template_retrieving\n",
    "    | final_prompt_truncation\n",
    "    | chat\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You will receive a question from a student taking a Tableau course, which includes a title and a body. \\nThe corresponding lecture will also be provided.\\n\\nAnswer the question using only the provided context.\\n\\nAt the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \\nResources: \\nSection: *Section Title*, Lecture: *Lecture Title*', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"This is the question:\\nLecture: Adding a custom calculation\\nTitle: Why are we using SUM here? It's unclear to me.\\nBody: This question refers to calculating the GM%.\\n\\nThis is the context:\\n\\nDocument 1\\nSection Title: Unknown Section\\nLecture Title: Unknown Lecture\\nContent: ## Adding a custom calculation \\nOk, excellent. \\nWe're doing good. \\nWe've seen quite a few interesting tableau \\ntools so far and we'll continue to do so during \\nthis lesson as well. \\nOur table is almost ready. \\nWe have revenue cogs and gross profit. \\nNow that I think about it, one thing we should \\nprobably add is a gross margin calculation \\nright next to the gross profit figures. \\nGross margin is useful because it allows us to \\nsee what portion of revenues were converted \\ninto gross profit once we have considered the \\ncost of goods sold. \\nRight. \\nLet's add a new calculated field. \\nI'll name it GM percent. \\nAll we have to do is divide gross profits by \\nrevenue, right? \\nAnd we already know how to do that. Ok. \\nHere we are. \\nWe've calculated a new field. \\nLet's add it to the table. \\nI'll insert it in the measure values card there. \\nI've added the new field right next to gross \\nprofit. \\nBut it looks strange, doesn't it? \\nIf we divide gross profit by revenues, we would \\nusually expect a number in the region of 10, \\n2030 or maybe 50%. \\nCertainly not 1000. \\nWhat is going on here when I divide 71 million \\nby 244 million, \\nwhich is what we have in January. \\nI obtain 29%. \\nApproximately. \\nThere are two possible explanations. \\nEither tableau miscalculated, the simple \\ndivision we asked it to perform or our formula \\nis not 100%. OK.\\n\\n-------------------\\n\\nDocument 2\\nSection Title: Unknown Section\\nLecture Title: Unknown Lecture\\nContent: It's most likely us and not the computer, right. \\nI'll take out the GM percent field from the \\nmeasure values box and we'll edit the \\ncalculated field from here. \\nWhat we forgot to do is type sum around the two \\nvariables. \\nIf we don't sum the variables, we are not \\ndividing their total figures for each month. \\nLet's adjust our calculated field in this way \\nand see what happens. \\nOK? \\nThis is a column with numbers that look like \\nzeros, but perhaps these are percentage values. \\nLet's change the way the GM percent \\ncolumn is displayed to do that. \\nI'll simply click on the GM percent variable in \\nthe measure values card and select the format \\noption. \\nWe have quite a few options available here. \\nSo I'll simply select a percentage format with \\none decimal place. \\nVoila. \\nOur table is ready in our next lesson. \\nWe'll add a filter that would allow us to \\nchoose whether to see both 2016 and 2017 \\nvalues or just one at a time. \\nSee you there. \\n \\n## Inserting a filter \\nOne of the most interesting options available \\nin tableaux is adding a filter to the \\nvisualization you are working on in this lesson. \\nWe'll add a filter to the \\nvisualization you are working on in this lesson. \\nWe'll add a filter to the \\nvisualization you are working on in this lesson. \\nWe'll add a filter to the \\nvisualization you are working on in this lesson. \\nWe'll add a filter to the \\nvisualization you are working on in this lesson. \\nWe'll add a filter to the \\nvisualization you\n"
     ]
    }
   ],
   "source": [
    "question_input = {\n",
    "    \"question_lecture\": \"Adding a custom calculation\",\n",
    "    \"question_title\": \"Why are we using SUM here? It's unclear to me.\",\n",
    "    \"question_body\": \"This question refers to calculating the GM%.\"\n",
    "}\n",
    "\n",
    "result = chain_retrieving_improved.invoke(question_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You will receive a question from a student taking a Tableau course, which includes a title and a body. \n",
      "The corresponding lecture will also be provided.\n",
      "\n",
      "Answer the question using only the provided context.\n",
      "\n",
      "At the end of your response, include the section and lecture names where the context was drawn from, formatted as follows: \n",
      "Resources: \n",
      "Section: *Section Title*, Lecture: *Lecture Title*', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"This is the question:\n",
      "Lecture: Adding a custom calculation\n",
      "Title: Why are we using SUM here? It's unclear to me.\n",
      "Body: This question refers to calculating the GM%.\n",
      "\n",
      "This is the context:\n",
      "\n",
      "Document 1\n",
      "Section Title: Unknown Section\n",
      "Lecture Title: Unknown Lecture\n",
      "Content: ## Adding a custom calculation \n",
      "Ok, excellent. \n",
      "We're doing good. \n",
      "We've seen quite a few interesting tableau \n",
      "tools so far and we'll continue to do so during \n",
      "this lesson as well. \n",
      "Our table is almost ready. \n",
      "We have revenue cogs and gross profit. \n",
      "Now that I think about it, one thing we should \n",
      "probably add is a gross margin calculation \n",
      "right next to the gross profit figures. \n",
      "Gross margin is useful because it allows us to \n",
      "see what portion of revenues were converted \n",
      "into gross profit once we have considered the \n",
      "cost of goods sold. \n",
      "Right. \n",
      "Let's add a new calculated field. \n",
      "I'll name it GM percent. \n",
      "All we have to do is divide gross profits by \n",
      "revenue, right? \n",
      "And we already know how to do that. Ok. \n",
      "Here we are. \n",
      "We've calculated a new field. \n",
      "Let's add it to the table. \n",
      "I'll insert it in the measure values card there. \n",
      "I've added the new field right next to gross \n",
      "profit. \n",
      "But it looks strange, doesn't it? \n",
      "If we divide gross profit by revenues, we would \n",
      "usually expect a number in the region of 10, \n",
      "2030 or maybe 50%. \n",
      "Certainly not 1000. \n",
      "What is going on here when I divide 71 million \n",
      "by 244 million, \n",
      "which is what we have in January. \n",
      "I obtain 29%. \n",
      "Approximately. \n",
      "There are two possible explanations. \n",
      "Either tableau miscalculated, the simple \n",
      "division we asked it to perform or our formula \n",
      "is not 100%. OK.\n",
      "\n",
      "-------------------\n",
      "\n",
      "Document 2\n",
      "Section Title: Unknown Section\n",
      "Lecture Title: Unknown Lecture\n",
      "Content: It's most likely us and not the computer, right. \n",
      "I'll take out the GM percent field from the \n",
      "measure values box and we'll edit the \n",
      "calculated field from here. \n",
      "What we forgot to do is type sum around the two \n",
      "variables. \n",
      "If we don't sum the variables, we are not \n",
      "dividing their total figures for each month. \n",
      "Let's adjust our calculated field in this way \n",
      "and see what happens. \n",
      "OK? \n",
      "This is a column with numbers that look like \n",
      "zeros, but perhaps these are percentage values. \n",
      "Let's change the way the GM percent \n",
      "column is displayed to do that. \n",
      "I'll simply click on the GM percent variable in \n",
      "the measure values card and select the format \n",
      "option. \n",
      "We have quite a few options available here. \n",
      "So I'll simply select a percentage format with \n",
      "one decimal place. \n",
      "Voila. \n",
      "Our table is ready in our next lesson. \n",
      "We'll add a filter that would allow us to \n",
      "choose whether to see both 2016 and 2017 \n",
      "values or just one at a time. \n",
      "See you there. \n",
      " \n",
      "## Inserting a filter \n",
      "One of the most interesting options available \n",
      "in tableaux is adding a filter to the \n",
      "visualization you are working on in this lesson. \n",
      "We'll add a filter to the \n",
      "visualization you are working on in this lesson. \n",
      "We'll add a filter to the \n",
      "visualization you are working on in this lesson. \n",
      "We'll add a filter to the \n",
      "visualization you are working on in this lesson. \n",
      "We'll add a filter to the \n",
      "visualization you are working on in this lesson. \n",
      "We'll add a filter to the \n",
      "visualization you\n"
     ]
    }
   ],
   "source": [
    "# Clean and format the final output\n",
    "def format_response_text(text: str) -> str:\n",
    "    return text.replace(\"\\\\n\", \"\\n\").replace(\"  \", \" \").strip()\n",
    "\n",
    "formatted_result = format_response_text(str(result))\n",
    "print(formatted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
